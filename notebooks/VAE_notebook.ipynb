{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variational Autoencoder (VAE) is a type of deep generative model that can learn to encode high-dimensional data, such as images, into a low-dimensional latent space and then decode that latent representation back to the original data space. A VAE is particularly useful in imaging data, as it can capture meaningful features in a compressed form, making it easier to analyze patterns, generate new images, or explore variations in the data.\n",
    "\n",
    "**What Does a Simple VAE Do?**\n",
    "* Encoder:\n",
    "\n",
    "    The encoder maps the input image into a latent space by compressing it into a lower-dimensional representation.\n",
    "    Unlike a traditional autoencoder, which might produce a fixed vector, the VAE encoder outputs two components for each latent dimension: a mean and a log variance. These parameters define a Gaussian distribution over the latent space for each input.\n",
    "\n",
    "* Latent Space Sampling:\n",
    "\n",
    "    After the encoder produces a mean and variance, a sample is drawn from this Gaussian distribution, which allows the VAE to introduce some randomness or variability into the latent representation.\n",
    "    The sampling process makes the VAE a generative model, enabling it to create new images by sampling different points in the latent space.\n",
    "\n",
    "* Decoder:\n",
    "\n",
    "    The sampled latent vector is then fed to the decoder, which reconstructs the image.\n",
    "    The decoder tries to reproduce the original input as accurately as possible, allowing the VAE to learn a compressed, yet informative, representation of the input data.\n",
    "* Loss Function:\n",
    "\n",
    "    The VAE optimizes two components:\n",
    "    Reconstruction Loss: Measures the similarity between the input image and the reconstructed image, encouraging the VAE to accurately capture image details.\n",
    "    KL Divergence: Regularizes the latent space, ensuring the learned latent distributions are close to a standard Gaussian. This keeps the latent space smooth, meaning that similar points in the latent space correspond to similar reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atomai import stat as atomstat\n",
    "import atomai as aoi\n",
    "\n",
    "import numpy as np\n",
    "import pyroved as pv\n",
    "\n",
    "import torch\n",
    "import random\n",
    "tt = torch.tensor\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import feature\n",
    "import skimage\n",
    "from scipy.ndimage import zoom\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize imagestack\n",
    "subimages_target = subimages_target/subimages_target.max()\n",
    "subimages_target = np.expand_dims(subimages_target, axis=-1)\n",
    "train_data = torch.tensor(subimages_target[:,:,:,0]).float()\n",
    "train_loader = pv.utils.init_dataloader(train_data.unsqueeze(1), batch_size=48, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, running the VAE in PyroVEd. Simple VAE will find the best representation of our data as two components for latent vecotr (l1,l2). Of course, we can explore other dimensinalities of latent space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = (window_size[0],window_size[1])\n",
    "\n",
    "# Initialize vanilla VAE\n",
    "vae = pv.models.iVAE(in_dim, latent_dim=2,   # Number of latent dimensions other than the invariancies\n",
    "                     hidden_dim_e = [512, 512],\n",
    "                     hidden_dim_d = [512, 512], # corresponds to the number of neurons in the hidden layers of the decoder\n",
    "                     invariances=None, seed=0)\n",
    "# Initialize SVI trainer\n",
    "trainer = pv.trainers.SVItrainer(vae)\n",
    "\n",
    "# Train for n epochs:\n",
    "for e in range(10):\n",
    "    trainer.step(train_loader)\n",
    "    trainer.print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ferro_VAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
